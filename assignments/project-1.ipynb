{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Machine Learning Project without Mllib Pipline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize SparkSession & Reading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female| 35|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male| 35|    0|    0|          373450|   8.05| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession or use the existing one\n",
    "spark = SparkSession.builder.appName(\"Titanic Data\").getOrCreate()\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"./data/titanic/train.csv\")\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting some columns (if needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-------+--------+\n",
      "|Survived|Pclass|   Sex| Age|   Fare|Embarked|\n",
      "+--------+------+------+----+-------+--------+\n",
      "|     0.0|   3.0|  male|22.0|   7.25|       S|\n",
      "|     1.0|   1.0|female|38.0|71.2833|       C|\n",
      "|     1.0|   3.0|female|26.0|  7.925|       S|\n",
      "|     1.0|   1.0|female|35.0|   53.1|       S|\n",
      "|     0.0|   3.0|  male|35.0|   8.05|       S|\n",
      "+--------+------+------+----+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "dataset = df.select(\n",
    "    col(\"Survived\").cast(\"float\"),\n",
    "    col(\"Pclass\").cast(\"float\"),\n",
    "    col(\"Sex\"),\n",
    "    col(\"Age\").cast(\"float\"),\n",
    "    col(\"Fare\").cast(\"float\"),\n",
    "    col(\"Embarked\"),\n",
    ")\n",
    "\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing null values (if needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+---+----+--------+\n",
      "|Survived|Pclass|Sex|Age|Fare|Embarked|\n",
      "+--------+------+---+---+----+--------+\n",
      "|       0|     0|  0|177|   0|       2|\n",
      "+--------+------+---+---+----+--------+\n",
      "\n",
      "+--------+------+---+---+----+--------+\n",
      "|Survived|Pclass|Sex|Age|Fare|Embarked|\n",
      "+--------+------+---+---+----+--------+\n",
      "|       0|     0|  0|  0|   0|       0|\n",
      "+--------+------+---+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "\n",
    "# Display the number of rows having null values inside each of the columns (1)\n",
    "dataset.select([count(when(isnull(c), c)).alias(c) for c in dataset.columns]).show()\n",
    "\n",
    "# Replace any occurrence of '?' with None and drop any rows having any None values\n",
    "dataset = dataset.replace(\"?\", None).dropna(how=\"any\")\n",
    "\n",
    "\n",
    "# Display the number of rows having null values inside each of the columns (2 - check the data frame again)\n",
    "dataset.select([count(when(isnull(c), c)).alias(c) for c in dataset.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting categorical variables to numeric values\n",
    "\n",
    "Spark only supports numeric values and is incapable of handling categorical variables.\n",
    "\n",
    "For modeling, all categorical variables must be converted to numeric values. To achieve this, StringIndexer is employed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-------+--------+------+-------+\n",
      "|Survived|Pclass|   Sex| Age|   Fare|Embarked|Gender|Boarded|\n",
      "+--------+------+------+----+-------+--------+------+-------+\n",
      "|     0.0|   3.0|  male|22.0|   7.25|       S|   0.0|    0.0|\n",
      "|     1.0|   1.0|female|38.0|71.2833|       C|   1.0|    1.0|\n",
      "+--------+------+------+----+-------+--------+------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "dataset = (\n",
    "    StringIndexer(inputCol=\"Sex\", outputCol=\"Gender\", handleInvalid=\"keep\")\n",
    "    .fit(dataset)\n",
    "    .transform(dataset)\n",
    ")\n",
    "dataset = (\n",
    "    StringIndexer(inputCol=\"Embarked\", outputCol=\"Boarded\", handleInvalid=\"keep\")\n",
    "    .fit(dataset)\n",
    "    .transform(dataset)\n",
    ")\n",
    "\n",
    "dataset.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-------+------+-------+\n",
      "|Survived|Pclass| Age|   Fare|Gender|Boarded|\n",
      "+--------+------+----+-------+------+-------+\n",
      "|     0.0|   3.0|22.0|   7.25|   0.0|    0.0|\n",
      "|     1.0|   1.0|38.0|71.2833|   1.0|    1.0|\n",
      "+--------+------+----+-------+------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.drop(\"Sex\").drop(\"Embarked\")\n",
    "\n",
    "dataset.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "Spark learns through two columns, <span style=\"color:green\">label</span> and <span style=\"color:green\">feature</span>.\n",
    "<span style=\"color:red\">Therefore, all columns except the target column must be combined into a single column.</span> This is accomplished via <span style=\"color:green\">VesctorAssembler</span>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-------+------+-------+--------------------+\n",
      "|Survived|Pclass| Age|   Fare|Gender|Boarded|             feature|\n",
      "+--------+------+----+-------+------+-------+--------------------+\n",
      "|     0.0|   3.0|22.0|   7.25|   0.0|    0.0|[3.0,22.0,7.25,0....|\n",
      "|     1.0|   1.0|38.0|71.2833|   1.0|    1.0|[1.0,38.0,71.2833...|\n",
      "|     1.0|   3.0|26.0|  7.925|   1.0|    0.0|[3.0,26.0,7.92500...|\n",
      "|     1.0|   1.0|35.0|   53.1|   1.0|    0.0|[1.0,35.0,53.0999...|\n",
      "|     0.0|   3.0|35.0|   8.05|   0.0|    0.0|[3.0,35.0,8.05000...|\n",
      "+--------+------+----+-------+------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler as va\n",
    "\n",
    "# Prepare features\n",
    "\n",
    "# Define the feature columns set\n",
    "original_feature_columns = [\"Pclass\", \"Age\", \"Fare\", \"Gender\", \"Boarded\"]\n",
    "\n",
    "# Define the vector assembler to map the feature set to be dimensionally reduced to the feature column\n",
    "assembler = va(inputCols=original_feature_columns, outputCol=\"feature\")\n",
    "# Use the VectorAssembler instance to transform the dataset in the spark required format\n",
    "spark_required_formatted_data = assembler.transform(dataset)\n",
    "\n",
    "spark_required_formatted_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 565\n",
      "Number of test samples: 147\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier as rfc\n",
    "\n",
    "# Splitting the dataset into train(80%) and test(20%) subsets\n",
    "training_data, testing_data = spark_required_formatted_data.randomSplit([0.8, 0.2])\n",
    "\n",
    "print(f\"Number of train samples: {str(training_data.count())}\")\n",
    "print(f\"Number of test samples: {str(testing_data.count())}\")\n",
    "\n",
    "# Instantiate the Random-Forest instance\n",
    "rf = rfc(labelCol=\"Survived\", featuresCol=\"feature\", maxDepth=5)\n",
    "\n",
    "# Train the rf model and save it in the model variable\n",
    "model = rf.fit(training_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.7755\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator as mcv\n",
    "\n",
    "evaluator = mcv(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Training set accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Spark jobs\n",
    "\n",
    "## The Jobs\n",
    "\n",
    "<img src=\"./images/spark-jobs-check.png\" style=\"width: 500px\">\n",
    "<img src=\"./images/spark-jobs-check-2.png\" style=\"width: 500px\">\n",
    "\n",
    "## The Stages\n",
    "\n",
    "<img src=\"./images/spark-stages-check.png\" style=\"width: 500px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop the Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
